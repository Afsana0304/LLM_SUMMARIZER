# LLM_SUMMARIZER
End-to-end LLM-powered text summarization app with FastAPI backend, Streamlit frontend, and local HuggingFace model integration.

A modular AI application for generating concise summaries from long texts using state-of-the-art Large Language Models (LLMs).  
The project features a FastAPI backend for serving the model, a Streamlit frontend for user interaction, and supports easy model management with local HuggingFace integration.

- ğŸ”„ FastAPI backend for robust, scalable API
- ğŸ“ Streamlit frontend for user-friendly input/output
- ğŸ¤— Model loading from local directory (downloaded via script)

## ğŸš€ Getting Started

1ï¸âƒ£ Clone the repo

git clone https://github.com/Afsana0304/LLM_SUMMARIZER.git
cd LLM_SUMMARIZER 

2ï¸âƒ£ Install dependencies

python -m venv venv
source venv/bin/activate(Mac)     # On Windows: venv\Scripts\activate
pip install -r requirements.txt

3ï¸âƒ£ Download the model
cd models
python download_model.py

4ï¸âƒ£ Run the backend (FastAPI)
cd Backend
uvicorn app.main:app --reload

5ï¸âƒ£ Run the frontend (Streamlit)
Open a new terminal:
cd Frontend
streamlit run app.py

ğŸ”– Notes
ğŸ—‚ï¸ Model files are gitignoredâ€”use models/download_model.py to fetch them.





